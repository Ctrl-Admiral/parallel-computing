# Лабораторная работа №2.
## Распараллеливание программы с помощью MPI

Работа выполнялась на вычислительном кластере НИУ ВШЭ cHARISMa: <https://hpc.hse.ru/hardware/hpc-cluster>.

### Условие задачи

Вариант 2 (6 по списку в группе). Задача нахождения среднего значения для заданного набора числовых данных. Реализовать с помощью MPI, произвести замеры.

### Сборка программы

Для сборки программы используется компилятор `mpiicpc`.

`module load INTEL/parallel_studio_xe_2020_u4_ce` -- для подключения необходимого для mpiicpc модуля.

`mpiicpc task2.cpp -O3 -o task2` -- сборка с уровнем оптимизации О3.

### Запуск программы

Для запуска необходимо создать `.sbatch` скрипт. Создадим `run.sbatch` со следующим содержанием:

```
#!/bin/bash

#SBATCH --job-name=run.sbatch              # Название задачи
#SBATCH --error=run.err                    # Файл для вывода ошибок
#SBATCH --output=run.log                   # Файл для вывода результатов
#SBATCH --time=00:05:00                    # Максимальное время выполнения
#SBATCH --ntasks=64                        # Количество MPI процессов
#SBATCH --ntasks-per-node=16               # Количество процессов на каждом узле
#SBATCH --nodes=4                          # Требуемое кол-во узлов
#SBATCH --gpus=0                           # Требуемое кол-во GPU

module load INTEL/parallel_studio_xe_2020_u4_ce
module load openmpi/4.0.5
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
srun /home/sigavarina/Projects/parallel-computing/task2
```

Запуск осуществляется командой `sbatch run.sbatch`.

### Реализация

Код с необходимыми комментариями представлен в файле `task2.cpp`.

Логика программы выглядит примерно следующим образом:

1. Инициализация MPI. В качестве мастер-процесса определяем нулевой.
2. Генерация случайного `std::vector<double>`. Т.к. `N = `, а double занимает в памяти 8 байт, то общий размер сгенерированного вектора равен 8 Гб. Этого достаточно, чтобы расходы на пересылку данных между нодами были меньше, чем выгода от такого распараллеливания.
3. Начинается отсчет времени.
4. Каждый процесс суммирует свою часть вектора. Так, если у нас 4 процесса, то нулевой считает сумму 0, 4, 8, ... элементов, первый -- сумму 1, 5, 9,... элементов и т.д.
5. Каждый процесс посылает мастер-процессу свою часть суммы. Мастер процесс суммирует все полученные значения, а далее вычисляет среднее значение. Предполагаем, что вычисление среднего выгоднее вычислять на одном процессе, это будет дешевле, чем рассылать вычисления на несколько нод.
6. Заканчивается отсчет времени. Выводятся результаты.

### Результаты

Результаты представлены в таблице ниже. Заголовки столбцов -- количество нод, на которых запускались процессы. Заголовки строк -- количество процессов.

| процессы/ноды |           1 |           4 |            16 |
|---------------|-------------|-------------|---------------|
| 1             | 1207012178  |             |               |
| 4             | 2438582767  |  8646633506 |               |
| 16            | 3070070045  | 79953366563 |  12573499682  |
| 64            |             |             |  74699115554  |
